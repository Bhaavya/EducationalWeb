A.2 Pseudo Counts, Smoothing, and Setting Hyperparameters : How can we interpret our result for a Bayesian estimate of binomial parameters? We know that the Beta and binomial distributions are similar. In fact, their relationship can be stated as the Beta distribution is the conjugate prior of the binomial distribution. All distributions in the exponential family have conjugate priors. The relationship is such: given a likelihood from an X distribution, picking the conjugate prior distribution of X (say it's Y ) will ensure that the posterior distribution is also a Y distribution. For our coin flipping case, the likelihood was a binomial distribution. We picked our prior to be the Beta distribution, and our posterior distribution ended up also being a Beta distribution-this is because we picked the conjugate prior! In any event, the whole reasoning behind having a prior is so we can include some reasonable guess for the parameters before we even see any data. For coin flipping, we might want to assume a "fair" coin. If for some reason we believe that the coin may be biased, we can incorporate that knowledge as well. If we look at the estimate for θ , we can imagine how setting our hyperparameters can influence our prediction. Recall that θ is the probability of heads; if we want to make our estimate biased toward more heads, we can set an α > β since this increases θ . This agrees with the mean of the prior as well: α α+β . Setting the mean equal to 0.8 means that our prior belief is a coin that lands heads 80% of the time. When used as a prior, we usually don't have any specific information about the individual indices in the Dirichlet. Because of this, we set them all to the same value. So instead of writing p(θ | α), where α is (e.g.) {0.1, 0.1, . . . , 0.1}, we simply say p(θ | α), where alpha is a scalar representing a vector of identical values. θ ∼ Dir(α) and θ ∼ Dir(0.1) are also commonplace, as is θ ∼ Beta(α, β) or θ ∼ Beta(0.4, 0.1). Figure A.2 shows how the choice of α characterizes the Dirichlet. The higher the area, the more likely a point (representing a vector) will be drawn. Let's take a moment to understand what a point drawn from the Dirichlet means. Look at the far right graph in Figure A.2. If the point we draw is from the peak of the graph, we'll get a multinomial parameter vector with a roughly equal proportion of each of the three components. For example, θ = {0.33, 0.33, 0.34}. With α = 10, it's very likely that we'll get a θ like this. In the middle picture, we're unsure what kind of θ we'll draw. It is equally likely to get an even mixture, uneven mixture, or anywhere in between. This is called a uniform prior-it can represent that we have no explicit information about the prior distribution. Finally, the plot on the left is a sparse prior (like a Beta where α, β < 1). Note: a uniform prior does not mean that we get an even mixture of components; it means it's equally likely to get any mixture. This could be confusing since the distribution we draw may actually be a uniform distribution. A sparse prior is actually quite relevant in a textual application; if we have a few dimensions with very high probability and the rest with relatively low occurrences, this should sound just like Zipf's law. We can use a Dirichlet prior to enforce a sparse word distribution per topic (θ = {0.9, 0.02, 0.08}). In topic modeling, we can use a Dirichlet distribution to force a sparse topic distribution per document. It's most likely that a document mainly discusses a handful of topics while the rest are A.5 Bayesian Estimate of Multinomial Parameters 463 largely unrepresented, just like the words the, to, of , and from are common while many words such as preternatural are rare. 