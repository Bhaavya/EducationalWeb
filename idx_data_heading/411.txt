A.5 Bayesian Estimate of Multinomial Parameters : Let's do parameter estimation with our multinomial distribution and relate it to the Beta-binomial model from before. For MLE, we would have (A.14) Using Bayes' rule, we represent the posterior as the product of the likelihood (multinomial) and prior (Dirichlet): We say these are proportional because we left out the constant of proportionality in the multinomial and Dirichlet distributions (the ratio with Gammas). We can now observe that the posterior is also a Dirichlet as expected due to the conjugacy. To actually obtain the Bayesian estimate, we'd need to fully substitute the multinomial and Dirichlet distributions into the posterior and integrate over all θ s to get our estimate. Since this isn't a note on calculus, we simply display the final answer as This looks very similar to the binomial estimation! We see the Dirichlet hyperparameters act as pseudo counts, smoothing our estimate. In Dirichlet prior smoothing for information retrieval, we have the formula: So we have x = c(w, d) and n = |d|, the count of the current word in a document and the length of the document respectively. Then we have α i = μp(w | C) and μ = k j =1 α j , the number of pseudo counts for word w and the total number of pseudo counts. Can you tell what the vector of hyperparameters for query likelihood 464 Appendix A Bayesian Statistics smoothing would be now? It's μ = μp(w 1 | C), μp(w 2 | C), . . . , μp(w k | C) (A.17) In other words, the Dirichlet prior for this smoothing method is proportional to the background, collection language model. Looking back at Add-1 smoothing, we can imagine this as a special case of Dirichlet prior smoothing. If we drew the uniform distribution from our Dirichlet, we'd get This implies that each word is equally likely in our collection language model, which is most likely not the case. Note |V | = k, or k i=1 1 since μ = {1, 1, . . . , 1}. 